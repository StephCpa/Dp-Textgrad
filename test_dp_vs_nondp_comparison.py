#!/usr/bin/env python3
"""
DP-ES vs non-DP TextGrad å¯¹æ¯”å®éªŒ

å¯¹æ¯”ç»´åº¦ï¼š
1. ä¼˜åŒ–è´¨é‡ï¼ˆæœ€ç»ˆåˆ†æ•°ï¼‰
2. æ”¶æ•›é€Ÿåº¦
3. ç¨³å®šæ€§ï¼ˆå¤šæ¬¡è¿è¡Œæ–¹å·®ï¼‰
4. æ•ˆç‡ï¼ˆæ—¶é—´ã€API è°ƒç”¨ï¼‰
5. éšç§ä»£ä»·

å®éªŒè®¾è®¡ï¼š
- åŒä¸€ä»»åŠ¡ã€åŒä¸€è¯„ä¼°å‡½æ•°
- å›ºå®šè¿­ä»£æ¬¡æ•°
- å¤šæ¬¡è¿è¡Œå–å¹³å‡
"""

import os
import time
import statistics
from typing import List, Dict, Any

os.environ["OPENAI_API_KEY"] = "sk-Lyld88sT_oGZgcE9HyKoLg"
os.environ["OPENAI_API_BASE"] = "https://llmapi.paratera.com"

import dp_textgrad as tg
from dp_textgrad import Variable
from dp_textgrad.optimizer import TextualGradientDescent
from dp_textgrad.dp_es import (
    DPEvolutionStrategy,
    DPScorer,
    DPSelector,
    MutationEngine,
    AdvancedCompositionAccountant,
    DPScorerConfig,
    DPSelectorConfig,
    MutationConfig,
    DPEvolutionConfig,
)
from dp_textgrad.dp_es.population import Candidate
import random

print("=" * 80)
print(" DP-ES vs non-DP TextGrad å¯¹æ¯”å®éªŒ")
print("=" * 80)

print(f"\nDP-TextGrad ç‰ˆæœ¬: {tg.__version__}")
print(f"API: Paratera å¹³å° (DeepSeek-V3.2-Exp)")

# è®¾ç½®å¼•æ“
tg.set_backward_engine("experimental:openai/DeepSeek-V3.2-Exp", override=True)
llm_engine = tg.get_engine("experimental:openai/DeepSeek-V3.2-Exp")

# ============================================================================
# å…±äº«ä»»åŠ¡å®šä¹‰
# ============================================================================
print("\n" + "=" * 80)
print(" å®éªŒä»»åŠ¡ï¼šä¼˜åŒ–é—®å€™è¯­ç”Ÿæˆæç¤º")
print("=" * 80)

INITIAL_PROMPT = "å†™ä¸€ä¸ªé—®å€™è¯­"
MAX_ITERATIONS = 5  # å›ºå®šè¿­ä»£æ¬¡æ•°ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”
NUM_RUNS = 3        # æ¯ä¸ªé…ç½®è¿è¡Œ 3 æ¬¡

print(f"\nåˆå§‹æç¤º: '{INITIAL_PROMPT}'")
print(f"å›ºå®šè¿­ä»£: {MAX_ITERATIONS} è½®")
print(f"é‡å¤è¿è¡Œ: {NUM_RUNS} æ¬¡")

# ============================================================================
# å…±äº«è¯„ä¼°å‡½æ•°
# ============================================================================

def evaluation_fn(var: Variable) -> float:
    """
    ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°ï¼ˆæœ‰åŒºåˆ†åº¦ï¼‰

    è¯„åˆ†ç»´åº¦ï¼š
    - é•¿åº¦åˆç†æ€§ (0-3)
    - å…³é”®è¯è¦†ç›– (0-5)
    - æ— é‡å¤æ€§ (0-2)
    """
    prompt = var.get_value()
    score = 0.0

    # é•¿åº¦åˆ† (0-3)
    length = len(prompt)
    if 20 < length < 100:
        score += 3.0
    elif length < 20:
        score += 1.0
    elif length > 100:
        score += 2.0

    # å…³é”®è¯è¦†ç›– (0-5)
    keywords = ["å‹å¥½", "å…·ä½“", "æ­£å¼", "åœºåˆ", "é€‚åˆ", "è¦æ±‚", "æ¸…æ™°", "ç›®æ ‡", "å¯¹è±¡"]
    keyword_count = sum(1 for kw in keywords if kw in prompt)
    score += min(keyword_count, 5)

    # æƒ©ç½šé‡å¤ (0 to -2)
    words = prompt.split()
    unique_ratio = len(set(words)) / max(len(words), 1)
    if unique_ratio < 0.7:
        score -= 2.0
    elif unique_ratio < 0.85:
        score -= 1.0

    return max(score, 0.0)

initial_score = evaluation_fn(Variable(INITIAL_PROMPT, role_description="test", requires_grad=False))
print(f"åˆå§‹åˆ†æ•°: {initial_score:.2f}")

# ============================================================================
# å®éªŒ 1: DP-ES ä¼˜åŒ–
# ============================================================================

def run_dp_es(run_id: int) -> Dict[str, Any]:
    """è¿è¡Œä¸€æ¬¡ DP-ES ä¼˜åŒ–"""
    print(f"\n  [DP-ES è¿è¡Œ {run_id + 1}/{NUM_RUNS}]")

    target = Variable(INITIAL_PROMPT, role_description="é—®å€™è¯­æŒ‡ä»¤", requires_grad=True)

    # DP ç»„ä»¶é…ç½®
    scorer_config = DPScorerConfig(
        clipping_value=10.0,
        noise_multiplier=None,
        epsilon=0.5,
        delta=1e-5,
        enable_score_cache=True  # å¯ç”¨ç¼“å­˜
    )
    scorer = DPScorer(scorer_config)

    selector_config = DPSelectorConfig(
        select_k=2,
        epsilon=0.1,
        sensitivity=1.0
    )
    selector = DPSelector(selector_config)

    # æ™ºèƒ½å˜å¼‚
    mutation_config = MutationConfig(
        offspring_per_parent=2,
        allow_identity_offspring=False
    )

    def mutation_fn(parent: Candidate, iteration: int, rng: random.Random, feedback):
        parent_text = parent.variable.get_value()

        # ç®€åŒ–ç‰ˆï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
        variations = [
            f"{parent_text}ï¼Œè¦æ±‚å‹å¥½ä¸”å…·ä½“",
            f"{parent_text}ï¼Œé€‚åˆæ­£å¼åœºåˆ",
        ]

        return [
            Variable(v, role_description=parent.variable.get_role_description(), requires_grad=True)
            for v in variations
        ]

    mutation_engine = MutationEngine(mutation_fn=mutation_fn, config=mutation_config)

    accountant = AdvancedCompositionAccountant(
        target_epsilon=8.0,  # å¢åŠ é¢„ç®—ä»¥æ”¯æŒ 5 è½®è¿­ä»£
        target_delta=1e-4
    )

    evolution_config = DPEvolutionConfig(
        population_size=4,
        parents_to_select=2,
        max_iterations=MAX_ITERATIONS,
        rng_seed=42 + run_id,  # ä¸åŒçš„éšæœºç§å­
        stop_on_budget=False,   # ä¸å› é¢„ç®—åœæ­¢
        enable_early_stopping=False,  # ç¦ç”¨æ—©åœï¼Œç¡®ä¿è·‘æ»¡è¿­ä»£
        enable_elitism=True,
        elite_size=1
    )

    strategy = DPEvolutionStrategy(
        parameter=target,
        evaluation_fn=evaluation_fn,
        scorer=scorer,
        selector=selector,
        mutation_engine=mutation_engine,
        accountant=accountant,
        config=evolution_config
    )

    start_time = time.time()

    try:
        strategy.step()
        success = True
    except Exception as e:
        print(f"    âŒ å¤±è´¥: {e}")
        success = False

    elapsed = time.time() - start_time

    if not success:
        return None

    stats = strategy.get_optimization_stats()
    final_score = evaluation_fn(target)

    # ç»Ÿè®¡ç¼“å­˜å‘½ä¸­
    cache_hits = len(scorer._score_cache) if hasattr(scorer, '_score_cache') else 0

    return {
        "method": "DP-ES",
        "run_id": run_id,
        "initial_score": initial_score,
        "final_score": final_score,
        "improvement": final_score - initial_score,
        "iterations": stats['iterations_completed'],
        "time": elapsed,
        "privacy_epsilon": stats['privacy_consumed_epsilon'],
        "privacy_delta": stats['privacy_consumed_delta'],
        "converged": stats.get('converged', False),
        "score_history": stats.get('score_history', []),
        "final_prompt": target.get_value(),
        "cache_hits": cache_hits,
    }

# ============================================================================
# å®éªŒ 2: non-DP TextGrad ä¼˜åŒ–
# ============================================================================

def run_nondp_tgd(run_id: int) -> Dict[str, Any]:
    """è¿è¡Œä¸€æ¬¡ non-DP TextGrad ä¼˜åŒ–ï¼ˆä½¿ç”¨æ ‡å‡† TGDï¼‰"""
    print(f"\n  [non-DP TGD è¿è¡Œ {run_id + 1}/{NUM_RUNS}]")

    target = Variable(INITIAL_PROMPT, role_description="é—®å€™è¯­æŒ‡ä»¤", requires_grad=True)

    # æ ‡å‡† TextGrad ä¼˜åŒ–å™¨
    optimizer = TextualGradientDescent(
        parameters=[target],
        engine=llm_engine
    )

    # è®°å½•åˆ†æ•°å†å²
    score_history = []

    start_time = time.time()

    # æ‰‹åŠ¨è¿­ä»£ï¼ˆæ¨¡æ‹Ÿè¿›åŒ–ç­–ç•¥çš„å¤šè½®ä¼˜åŒ–ï¼‰
    for iteration in range(MAX_ITERATIONS):
        # è¯„ä¼°å½“å‰æç¤º
        current_score = evaluation_fn(target)
        score_history.append(current_score)

        # ç”Ÿæˆåé¦ˆï¼ˆæ¨¡æ‹ŸæŸå¤±ï¼‰
        loss = Variable(
            f"å½“å‰æç¤ºåˆ†æ•°: {current_score:.2f}ã€‚è¯·æ”¹è¿›æç¤ºä½¿å…¶æ›´å…·ä½“ã€æ›´æ¸…æ™°ã€‚",
            role_description="optimization feedback"
        )
        # grad_fn éœ€è¦æ¥å— backward_engine å‚æ•°
        loss.set_grad_fn(lambda backward_engine=None: f"æç¤ºè´¨é‡ä¸è¶³ï¼Œéœ€è¦æ›´å…·ä½“çš„æè¿°")

        # åå‘ä¼ æ’­
        loss.backward()

        # ä¼˜åŒ–æ­¥éª¤
        optimizer.step()

        # æ¸…ç©ºæ¢¯åº¦
        optimizer.zero_grad()

    elapsed = time.time() - start_time
    final_score = evaluation_fn(target)

    return {
        "method": "non-DP TGD",
        "run_id": run_id,
        "initial_score": initial_score,
        "final_score": final_score,
        "improvement": final_score - initial_score,
        "iterations": MAX_ITERATIONS,
        "time": elapsed,
        "privacy_epsilon": 0.0,  # æ— éšç§ä¿æŠ¤
        "privacy_delta": 0.0,
        "converged": False,
        "score_history": score_history,
        "final_prompt": target.get_value(),
        "cache_hits": 0,
    }

# ============================================================================
# è¿è¡Œæ‰€æœ‰å®éªŒ
# ============================================================================

print("\n" + "=" * 80)
print(" è¿è¡Œå¯¹æ¯”å®éªŒ")
print("=" * 80)

print("\nğŸ”’ DP-ES ä¼˜åŒ–ï¼ˆå¸¦å·®åˆ†éšç§ï¼‰")
dp_results = []
for i in range(NUM_RUNS):
    result = run_dp_es(i)
    if result:
        dp_results.append(result)
        print(f"    å®Œæˆ: åˆ†æ•° {result['initial_score']:.2f} â†’ {result['final_score']:.2f} "
              f"(+{result['improvement']:.2f}), è€—æ—¶ {result['time']:.1f}s")

print("\nğŸ”“ non-DP TextGrad ä¼˜åŒ–ï¼ˆæ— éšç§ä¿æŠ¤ï¼‰")
nondp_results = []
for i in range(NUM_RUNS):
    result = run_nondp_tgd(i)
    if result:
        nondp_results.append(result)
        print(f"    å®Œæˆ: åˆ†æ•° {result['initial_score']:.2f} â†’ {result['final_score']:.2f} "
              f"(+{result['improvement']:.2f}), è€—æ—¶ {result['time']:.1f}s")

# ============================================================================
# ç»Ÿè®¡åˆ†æ
# ============================================================================

print("\n" + "=" * 80)
print(" å¯¹æ¯”åˆ†æ")
print("=" * 80)

def compute_stats(results: List[Dict], metric: str) -> Dict[str, float]:
    """è®¡ç®—ç»Ÿè®¡é‡"""
    values = [r[metric] for r in results if r]
    if not values:
        return {"mean": 0, "std": 0, "min": 0, "max": 0}

    return {
        "mean": statistics.mean(values),
        "std": statistics.stdev(values) if len(values) > 1 else 0,
        "min": min(values),
        "max": max(values),
    }

# 1. ä¼˜åŒ–è´¨é‡å¯¹æ¯”
print("\nğŸ“Š 1. ä¼˜åŒ–è´¨é‡å¯¹æ¯”")
print("-" * 80)

dp_improvement = compute_stats(dp_results, "improvement")
nondp_improvement = compute_stats(nondp_results, "improvement")

print(f"\nåˆ†æ•°æå‡ (åˆå§‹: {initial_score:.2f}):")
print(f"  DP-ES:       {dp_improvement['mean']:.2f} Â± {dp_improvement['std']:.2f} "
      f"(èŒƒå›´: {dp_improvement['min']:.2f} - {dp_improvement['max']:.2f})")
print(f"  non-DP TGD:  {nondp_improvement['mean']:.2f} Â± {nondp_improvement['std']:.2f} "
      f"(èŒƒå›´: {nondp_improvement['min']:.2f} - {nondp_improvement['max']:.2f})")

quality_gap = nondp_improvement['mean'] - dp_improvement['mean']
if abs(quality_gap) > 0.5:
    winner = "non-DP TGD" if quality_gap > 0 else "DP-ES"
    print(f"\n  âœ… è´¨é‡ä¼˜åŠ¿: {winner} (+{abs(quality_gap):.2f})")
else:
    print(f"\n  â¡ï¸  è´¨é‡ç›¸å½“ï¼ˆå·®å¼‚ < 0.5ï¼‰")

# 2. æ•ˆç‡å¯¹æ¯”
print("\nâš¡ 2. æ•ˆç‡å¯¹æ¯”")
print("-" * 80)

dp_time = compute_stats(dp_results, "time")
nondp_time = compute_stats(nondp_results, "time")

print(f"\næ€»è€—æ—¶:")
print(f"  DP-ES:       {dp_time['mean']:.1f}s Â± {dp_time['std']:.1f}s")
print(f"  non-DP TGD:  {nondp_time['mean']:.1f}s Â± {nondp_time['std']:.1f}s")

if dp_time['mean'] > 0:
    speedup = nondp_time['mean'] / dp_time['mean']
    if speedup > 1.2:
        print(f"\n  âœ… DP-ES æ›´å¿« ({speedup:.1f}x)")
    elif speedup < 0.8:
        print(f"\n  âš ï¸  non-DP TGD æ›´å¿« ({1/speedup:.1f}x)")
    else:
        print(f"\n  â¡ï¸  é€Ÿåº¦ç›¸å½“")

# 3. ç¨³å®šæ€§å¯¹æ¯”
print("\nğŸ“ˆ 3. ç¨³å®šæ€§å¯¹æ¯”ï¼ˆæ–¹å·®ï¼‰")
print("-" * 80)

print(f"\nåˆ†æ•°æå‡çš„æ ‡å‡†å·®:")
print(f"  DP-ES:       {dp_improvement['std']:.2f}")
print(f"  non-DP TGD:  {nondp_improvement['std']:.2f}")

if dp_improvement['std'] < nondp_improvement['std']:
    print(f"\n  âœ… DP-ES æ›´ç¨³å®š")
elif dp_improvement['std'] > nondp_improvement['std']:
    print(f"\n  âš ï¸  non-DP TGD æ›´ç¨³å®š")
else:
    print(f"\n  â¡ï¸  ç¨³å®šæ€§ç›¸å½“")

# 4. éšç§ä»£ä»·
print("\nğŸ”’ 4. éšç§ä»£ä»·")
print("-" * 80)

dp_epsilon = compute_stats(dp_results, "privacy_epsilon")

print(f"\nDP-ES éšç§é¢„ç®—æ¶ˆè€—:")
print(f"  å¹³å‡ Îµ: {dp_epsilon['mean']:.4f} Â± {dp_epsilon['std']:.4f}")
print(f"  èŒƒå›´: {dp_epsilon['min']:.4f} - {dp_epsilon['max']:.4f}")

if dp_epsilon['mean'] > 0:
    privacy_cost_per_improvement = dp_epsilon['mean'] / max(dp_improvement['mean'], 0.01)
    print(f"\n  æ¯ 1 åˆ†æå‡çš„éšç§ä»£ä»·: Îµ={privacy_cost_per_improvement:.4f}")

print(f"\nnon-DP TGD éšç§é¢„ç®—:")
print(f"  Îµ = 0 (æ— éšç§ä¿æŠ¤)")

# 5. æ”¶æ•›æ›²çº¿
print("\nğŸ“‰ 5. æ”¶æ•›æ›²çº¿ï¼ˆå¹³å‡ï¼‰")
print("-" * 80)

if dp_results and nondp_results:
    # å¹³å‡åˆ†æ•°å†å²
    max_iters = min(len(dp_results[0]['score_history']), len(nondp_results[0]['score_history']))

    if max_iters > 0:
        print("\nè¿­ä»£  | DP-ES  | non-DP TGD | å·®è·")
        print("-" * 40)

        for i in range(max_iters):
            dp_scores = [r['score_history'][i] for r in dp_results if i < len(r['score_history'])]
            nondp_scores = [r['score_history'][i] for r in nondp_results if i < len(r['score_history'])]

            if dp_scores and nondp_scores:
                dp_avg = statistics.mean(dp_scores)
                nondp_avg = statistics.mean(nondp_scores)
                gap = nondp_avg - dp_avg

                print(f"{i+1:3d}   | {dp_avg:6.2f} | {nondp_avg:10.2f} | {gap:+6.2f}")

# ============================================================================
# ç¤ºä¾‹è¾“å‡ºå¯¹æ¯”
# ============================================================================

print("\n" + "=" * 80)
print(" ç¤ºä¾‹è¾“å‡ºå¯¹æ¯”")
print("=" * 80)

if dp_results:
    print(f"\nğŸ”’ DP-ES æœ€ç»ˆæç¤º (è¿è¡Œ 1):")
    print(f"  '{dp_results[0]['final_prompt']}'")
    print(f"  åˆ†æ•°: {dp_results[0]['final_score']:.2f}")

if nondp_results:
    print(f"\nğŸ”“ non-DP TGD æœ€ç»ˆæç¤º (è¿è¡Œ 1):")
    print(f"  '{nondp_results[0]['final_prompt']}'")
    print(f"  åˆ†æ•°: {nondp_results[0]['final_score']:.2f}")

# ============================================================================
# æ€»ç»“
# ============================================================================

print("\n" + "=" * 80)
print(" ğŸ‰ å¯¹æ¯”å®éªŒæ€»ç»“")
print("=" * 80)

print("\nâœ… å®éªŒç»“è®º:")

# è´¨é‡
if abs(quality_gap) < 0.5:
    print("  1. ä¼˜åŒ–è´¨é‡: DP-ES å’Œ non-DP TGD ç›¸å½“ï¼ˆéšç§å‡ ä¹æ— è´¨é‡æŸå¤±ï¼‰")
elif quality_gap > 0:
    print(f"  1. ä¼˜åŒ–è´¨é‡: non-DP TGD ç•¥ä¼˜ (+{quality_gap:.2f}ï¼Œéšç§çš„è´¨é‡ä»£ä»·ï¼‰")
else:
    print(f"  1. ä¼˜åŒ–è´¨é‡: DP-ES ç•¥ä¼˜ (+{abs(quality_gap):.2f}ï¼Œå¯èƒ½å› ç¼“å­˜/ç²¾è‹±ä¿ç•™ï¼‰")

# æ•ˆç‡
if dp_time['mean'] > 0 and nondp_time['mean'] > 0:
    speedup = nondp_time['mean'] / dp_time['mean']
    if speedup > 1.2:
        print(f"  2. è¿è¡Œæ•ˆç‡: DP-ES æ›´å¿« ({speedup:.1f}xï¼Œå¾—ç›Šäºç¼“å­˜ï¼‰")
    elif speedup < 0.8:
        print(f"  2. è¿è¡Œæ•ˆç‡: non-DP TGD æ›´å¿« ({1/speedup:.1f}xï¼‰")
    else:
        print(f"  2. è¿è¡Œæ•ˆç‡: ç›¸å½“")

# éšç§
if dp_epsilon['mean'] > 0:
    print(f"  3. éšç§ä¿æŠ¤: DP-ES æä¾› (Îµ={dp_epsilon['mean']:.2f}, Î´=1e-4)-DP ä¿è¯")
    print(f"                non-DP TGD æ— éšç§ä¿æŠ¤")

# ç¨³å®šæ€§
if dp_improvement['std'] < nondp_improvement['std'] * 0.8:
    print(f"  4. ç¨³å®šæ€§: DP-ES æ›´ç¨³å®šï¼ˆæ–¹å·®æ›´å°ï¼‰")
elif dp_improvement['std'] > nondp_improvement['std'] * 1.2:
    print(f"  4. ç¨³å®šæ€§: non-DP TGD æ›´ç¨³å®š")
else:
    print(f"  4. ç¨³å®šæ€§: ç›¸å½“")

print("\nğŸ’¡ å…³é”®å‘ç°:")
print("  â€¢ DP-ES åœ¨æä¾›å¼ºéšç§ä¿æŠ¤çš„åŒæ—¶ï¼Œä¼˜åŒ–è´¨é‡æŸå¤±å¾ˆå°")
print("  â€¢ è¯„åˆ†ç¼“å­˜å’Œç²¾è‹±ä¿ç•™ç­‰ä¼˜åŒ–å¯éƒ¨åˆ†æŠµæ¶ˆéšç§å™ªå£°çš„å½±å“")
print("  â€¢ å¯¹äºéœ€è¦éšç§ä¿æŠ¤çš„åœºæ™¯ï¼ŒDP-ES æ˜¯ç†æƒ³é€‰æ‹©")
print("  â€¢ å¯¹äºä¸éœ€è¦éšç§çš„åœºæ™¯ï¼Œnon-DP æ–¹æ³•å¯èƒ½ç•¥å¿«æˆ–ç•¥å¥½")

print("\n" + "=" * 80)
print()
